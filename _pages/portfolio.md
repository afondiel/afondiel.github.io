---
permalink: /portfolio/
title: "Portfolio"
toc: true
toc_label: "Table of Contents"
toc_icon: "bookmark"
---
<!-- *Updated: 01/24/2025* -->

Explore my cutting-edge computer vision solutions addressing real-world challenges in smart industries such as autonomous systems, healthcare, agriculture, and more. 

### Core Projects: Real-World Computer Vision Applications
```
â”œâ”€â”€ End-to-End Computer Vision Stack            
â”‚   â”œâ”€â”€ Data Acquisition (Camera modeling & Calibration)
â”‚   â”œâ”€â”€ Modeling        
â”‚   â”œâ”€â”€ Optimization                  
â”‚   â”œâ”€â”€ Deployment (Cloud, Edge Devices)        
â”‚   â””â”€â”€ Monitoring
â”œâ”€â”€ Computer Vision Applications
â”‚   â”œâ”€â”€ Classification
â”‚   â”œâ”€â”€ Object Detection (2D, 3D, ..., ND)        
â”‚   â”œâ”€â”€ Segmentation (semantic, instance, panoptic)                   
â”‚   â”œâ”€â”€ Multimodality (VLMs: GANs, VAEs, Diffusers ...)        
â”‚   â””â”€â”€ Motion: Tracking & Flow
â”œâ”€â”€ Perception
â”‚   â”œâ”€â”€ Sensor Fusion (Camera, LiDAR, RADAR ...)                   
â”‚   â”œâ”€â”€ Depth Perception                                                     
â”‚   â””â”€â”€ VSLAM (VO) 
â””â”€â”€ Smart Cities
```
### Industries: Case Studies
```
â”œâ”€â”€ Autonomous Systems           
â”‚   â”œâ”€â”€ Robotics                 # Mobile, Humanoids, Embodied Agents
â”‚   â”œâ”€â”€ Self-Driving Cars        # ADAS, Autonomous Vehicles
â”‚   â””â”€â”€ UAS: Drones, sUAS, UAVs  # Unmanned Aerial Systems
â”œâ”€â”€ Healthcare 
â”‚   â””â”€â”€ Medical Imaging 
â”œâ”€â”€ Agriculture   
â”‚   â””â”€â”€ Precision Farming
â”œâ”€â”€ Security
â”‚   â””â”€â”€ Surveillance
â”œâ”€â”€ Manufacturing 
â”‚   â””â”€â”€ Quality Control
â”œâ”€â”€ Retail 
â”‚   â””â”€â”€ Consumer Analytics
â””â”€â”€ Smart Cities
    â””â”€â”€ Urban Planning
```

## Computer Vision
### Computer Vision Challenge ğŸ†

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/computer-vision-challenge)

A hands-on collection of foundational computer vision projects for everyone.
- **Level 0** - Zero/beginner: Getting Started with Basics
- **Level 1** - Apprentice/intermediate: Hands-on Computer Vision with Deep Learning
- **Level 2** - Hero: Large Vision Models (LVMs) from Image Generation, Inpainting, & More
- **Level 3** - Advanced: Video Models Benchmarking (ongoing)
- **Level 4** - Expert: Finetuning of VLMs (Vision Language Models) & LVMs (ongoing)
- **Level 5** - Master: Multimodality (ongoing)

## Perception

### Depth Perception for Obstacle Detection on the Road (From Self-Driving Car Specialization)

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/Self-Driving-Cars-Specialization/blob/7a3866e06e9e0e9ff9f010fd44e635e385f4514b/Course3-Visual-Perception-for-Self-Driving-Cars/resources/w1/lab/Applying%20Stereo%20Depth%20to%20a%20Driving%20Scenario%20(practice%20assignment).ipynb)

Implementing stereo depth estimation using Python and OpenCV on CARLA simulator images to calculate collision distances in a driving scenario.

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="/assets/images/portfolio_assets/depth_cover.png" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

**Additional Resources:** Learn core concepts [here](https://github.com/afondiel/Self-Driving-Cars-Specialization/blob/7a3866e06e9e0e9ff9f010fd44e635e385f4514b/Course3-Visual-Perception-for-Self-Driving-Cars/course3-w1-notes.md).

### Vision Odometry (VO) for Self-Driving Car Location

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/diesimo-ai/self-driving-car-projects/tree/main/p6-visual-odometry-for-localization)

This is a visual odometry system that estimates the vehicle's trajectory using visual data captured by its monocular camera.

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="https://github.com/diesimo-ai/self-driving-car-projects/blob/main/p6-visual-odometry-for-localization/doc/pair-imgs-pxls.png?raw=true" style="max-width: 100%; height: auto;" width="1280" height="720"></p>


## Case Studies

### Real-time Segmentation Deployment with Qualcomm AI Hub 

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/computer-science-notebook/blob/master/core/systems/edge-computing/edge-ai/lab/examples/deploy-with-qualcomm/notebooks/Deploy_RT_Segmentation_Model_On_Real_Device.ipynb) [![Open notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/afondiel/afondiel.github.io/blob/main/assets/images/blog-posts/edge-ai/101/lab/Deploy_RT_Segmentation_Model_On_Real_Device.ipynb)

<!-- STAR Framework => Result/value - Driven-->
This case study demonstrates how to deploy a semantic segmentation model optimized for edge devices using [Qualcomm AI Hub](https://aihub.qualcomm.com/). The example leverages **FFNet**, a model tailored for efficient edge-based semantic segmentation, tested on the [Cityscapes dataset](https://www.cityscapes-dataset.com/).

**Industry Applications**: Autonomous Driving, Augmented Reality, and Mobile Robotics

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="/assets/images/blog-posts/edge-ai/101/ffnet-seg.png" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

**Additional Resources**: [Blog](https://afondiel.github.io/posts/the-next-ai-frontier-is-at-the-edge/), [Ready-to-Deploy vision models for Edge Devices](https://github.com/afondiel/Edge-AI-Model-Zoo). 


### Self-Driving Car Environment Perception

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/diesimo-ai/self-driving-car-projects/tree/main/p7-self-driving-car-environment-perception)

Self-Driving Car foundational perception stack, which extracts useful information from its surroudings and perform complex tasks in order to drive safely through the world

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="https://github.com/diesimo-ai/self-driving-car-projects/blob/main/p7-self-driving-car-environment-perception/doc/final-project-segNet.png?raw=true" style="max-width: 100%; height: auto;" width="1280" height="720"></p>


## More Projects

- [Here](https://github.com/diesimo-ai).
