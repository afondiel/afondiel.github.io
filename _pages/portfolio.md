---
permalink: /portfolio/
title: "Portfolio"
toc: true
toc_label: "Table of Contents"
toc_icon: "bookmark"
---
<!-- *Updated: 01/24/2025* -->

Explore my cutting-edge computer vision solutions addressing real-world challenges in smart industries such as autonomous systems, healthcare, agriculture, and more. 

## Computer Vision Real-World Applications

### Computer Vision Challenge üèÜ

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/computer-vision-challenge) [![Demo on HF Spaces](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm-dark.svg)](https://huggingface.co/spaces/afondiel/image-colorizer-deoldify)

A hands-on collection of computer vision projects designed for all skill levels‚Äîbeginners to experts.

**Challenge Levels**
- **Level 0** - Beginner: Basics of image processing and OpenCV.
- **Level 1** - Intermediate: Build deep learning models with PyTorch/TensorFlow.
- **Level 2** - Hero: Explore image generation and inpainting with LVMs.
- **Level 3** - Advanced (ongoing): Benchmark video models for action recognition.
- **Level 4** - Expert (ongoing): Fine-tune VLMs and LVMs for custom datasets.
- **Level 5** - Master (ongoing): Build multimodal AI systems combining vision, text, and more. 

## Perception

### Multimodal Sensor Fusion with GPS, IMU, and LiDAR for Vehicule Localization.

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/diesimo-ai/self-driving-car-projects/tree/main/p5-self-driving-vehicle-state-estimation-on-roadway)

This project involved integrating data from multiple sensors to accurately determine a vehicle's position and motion on the roadway. The system uses techniques such as Kalman filtering to combine inputs from GPS, IMU, and LiDAR, enhancing the precision of state estimation critical for autonomous driving applications.

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="https://github.com/diesimo-ai/self-driving-car-projects/blob/main/p5-self-driving-vehicle-state-estimation-on-roadway/doc/full-state-estimation-pipeline.png?raw=true" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

**Additional Resources:** [Linear/Non Linear KF Implementation](https://github.com/afondiel/computer-science-notebook/blob/master/core/fundamentals/signal-processing/sp-algorithms/Kalman_filter_linear_nonlinear.ipynb).

### Depth Perception for Obstacle Detection on the Road

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/Self-Driving-Cars-Specialization/blob/7a3866e06e9e0e9ff9f010fd44e635e385f4514b/Course3-Visual-Perception-for-Self-Driving-Cars/resources/w1/lab/Applying%20Stereo%20Depth%20to%20a%20Driving%20Scenario%20(practice%20assignment).ipynb)

Implemented stereo depth estimation using Python and OpenCV on CARLA simulator images to calculate collision distances in a driving scenario.

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="/assets/images/portfolio_assets/depth_cover.png" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

**Additional Resources:** Learn core concepts [here](https://github.com/afondiel/Self-Driving-Cars-Specialization/blob/7a3866e06e9e0e9ff9f010fd44e635e385f4514b/Course3-Visual-Perception-for-Self-Driving-Cars/course3-w1-notes.md).

### Visual Odometry (VO) for Self-Driving Car Location

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/diesimo-ai/self-driving-car-projects/tree/main/p6-visual-odometry-for-localization)

This is a visual odometry system that estimates the vehicle's trajectory using realtime visual data captured by its (monocular) camera.

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="https://github.com/diesimo-ai/self-driving-car-projects/blob/main/p6-visual-odometry-for-localization/doc/pair-imgs-pxls.png?raw=true" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

## Edge AI

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/computer-science-notebook/tree/master/core/systems/edge-computing/edge-ai/lab/examples)

**Edge AI** involves processing data locally on devices, reducing inference cost, offering faster decision-making, and enhanced security.

- Learn more: [The Next AI Frontier is at the Edge](https://afondiel.github.io/posts/the-next-ai-frontier-is-at-the-edge/)

### Real-time Segmentation Deployment with Qualcomm AI Hub 

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/afondiel/computer-science-notebook/blob/master/core/systems/edge-computing/edge-ai/lab/examples/deploy-with-qualcomm/notebooks/Deploy_RT_Segmentation_Model_On_Real_Device.ipynb) [![Open notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/afondiel/afondiel.github.io/blob/main/assets/images/blog-posts/edge-ai/101/lab/Deploy_RT_Segmentation_Model_On_Real_Device.ipynb)

<!-- STAR Framework => Result/value - Driven-->
This case study demonstrates how to deploy a semantic segmentation model optimized for edge devices using [Qualcomm AI Hub](https://aihub.qualcomm.com/). The example leverages **FFNet**, a model tailored for efficient edge-based semantic segmentation, tested on the [Cityscapes dataset](https://www.cityscapes-dataset.com/).

**Industry Applications**: Autonomous Driving, Augmented Reality, and Mobile Robotics

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="/assets/images/blog-posts/edge-ai/101/ffnet-seg.png" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

## Case Studies

### Self-Driving Car Environment Perception

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/diesimo-ai/self-driving-car-projects/tree/main/p7-self-driving-car-environment-perception)

Self-Driving Car foundational perception stack, which extracts useful information from its surroudings and perform complex tasks in order to drive safely through the world

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="https://github.com/diesimo-ai/self-driving-car-projects/blob/main/p7-self-driving-car-environment-perception/doc/final-project-segNet.png?raw=true" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

### End-to-End Self-Driving Car Behavioral Cloning

[![View on GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/diesimo-ai/self-driving-car-behavioral-cloning/tree/master)

End-to-End self-driving car behavioral cloning implementation based on NVIDIA End-to-End Learning [paper](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) using computer vision, deep learning,  and realtime visual data from [Udacity Self-Driving Car simulator](https://github.com/udacity/self-driving-car-sim).

<p style="text-align: center;"> <img loading="lazy" decoding="async" class="aligncenter size-full" src="https://github.com/diesimo-ai/self-driving-car-behavioral-cloning/blob/master/docs/img/autonomous-mode.png?raw=true" style="max-width: 100%; height: auto;" width="1280" height="720"></p>

## More Projects & Solutions

- Explore more [here](https://github.com/diesimo-ai).

<div style="margin-top: 20px; text-align: center;">
    <a href="https://bit.ly/3ZGRLyo" class="btn btn--primary" style="margin: 0 10px;">Let's Chat</a>
    <a href="/services/" class="btn btn--primary" style="margin: 0 10px;">Expertises</a>
</div>

